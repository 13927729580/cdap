.. :author: Cask Data, Inc.
   :description: Architecture of the Cask Data Application Platform
     :copyright: Copyright © 2014 Cask Data, Inc.

========================================================
Cask Data Application Platform Architecture and Concepts
========================================================

Introduction
============

Before you learn how to write applications on CDAP, this section will introduce its concepts and architecture.

Virtualization
==============

CDAP lets you virtualize your data and applications by injecting abstraction layers over the various components
of the Hadoop ecosystem. To access and manipulate data, you interact with CDAP's Datasets rather than actual
storage engines such as HDFS or HBase. Similarly, you write your applications using CDAP's application interfaces
and run them inside application containers. These containers are a logical abstraction that can be realized
differently in several runtime environments, such as in-memory, single-node standalone, or distributed cluster,
without the need to change a single line of application code.

<Diagram of virtualization layers>

Data Virtualization
-------------------

In CDAP applications, you interact with data through Datasets. Datasets provide virtualization through:

- Abstraction of the actual representation of data in storage. You can write your code or queries without
  having to know where and how your data is stored - be it in HBase, LevelDB or a relational database.
- Encapsulation of data access patterns and business logic as reusable, managed Java code.
- Consistency of your data under highly concurrent access using CDAP's transaction system.
- Injection of datasets into different programming paradigms and runtimes: As soon as your data is in a
  dataset, you can instantly use it in real-time programs, batch processing such as Map/Reduce or Spark,
  and ad-hoc SQL queries.

Application Virtualization
--------------------------

CDAP's programming interfaces have multiple implementations in different runtime environments. You build
your applications against the CDAP application API. This API not only hides the low-level details of
individual programming paradigms and runtimes, it also gives access to many useful services provided by
CDAP, such as the dataset service or the service discovery. When you deploy and run the application into a
specific installation of CDAP, the appropriate implementations of all services and program runtimes are
injected by CDAP - the application does not need to change based on the environment. This allows you
develop applications in one environent - say, on your laptop using a stand-alone CDAP for testing - and
then seamlessly deploy it in a different environemnt - say, your distributed staging cluster.


Components of CDAP and their Interactions
=========================================

CDAP consists mainly of these components:

- The Router is the only public access point into CDAP for external clients. It forwards client requests to
  the appropriate system service or application. In a secure setup, the router also performs authentication;
  It is then complemented by an authentication service that allows clients to obtain credentials for CDAP.
- The Master controls and manages all services and applications.
- System Services provide vital platform features such datasets, transactions, service discovery logging,
  and metrics collection. System services run in application containers.
- Application containers provide virtualization and isolation for execution of application code (and, as a
  special case, system services). Application containers scale linearly and elastically with the underlying
  infrastructure.

.. image:: _images/architecture_diagram_1.png
:width: 6in
   :align: center

CDAP Programming Interfaces
===========================

We distinguish between the Developer interface and the Platform interface.

- The Developer interface is used to build applications and exposes various Java APIs that are only available to
  code that runs inside application containers, for example the Dataset and Transaction APIs as well as the
  various supported programming paradigms.
- The Platform interface is a RESTful API and the only way that external clients can interact with CDAP and
  applications. It includes APIs that are not accessible from inside containers, such as application
  management and monitoring. As an alternative to HTTP, clients can also use the client libraries
  provided for different programming languages, including Java, JavaScript and Python.

Note that some interfaces are included in both the Developer and the Platform APIs, for example, Service Discovery
and Dataset Management.

<venn diagram of APIs>



The Challenge of Big Data Applications
======================================

The amount of data being generated by businesses and consumers is compounding exponentially.
Applications are becoming increasingly complex and data-intensive as developers try to
extract value from this enormous trove of information. These applications—Big Data
applications—need to scale with the unpredictable volume and velocity of incoming data
without the need for the developer to re-architect the deployment infrastructure—even
while dealing with hundreds of petabytes if not exabytes of data. Building Big Data
applications is challenging on many fronts.

Steep learning curve
--------------------

As an application developer building a Big Data application,
you are primarily concerned with four areas:

 #. Data collection framework
 #. Data processing framework
 #. Data storage framework
 #. Data serving framework

There are many technology frameworks from which to choose in each of these four areas;
data storage alone runs the gamut from open-source NoSQL projects to proprietary
relational databases and can require you to learn CAP theorem concepts and understand
distributed systems principles. Evaluating the pros and cons of each of these frameworks,
becoming competent, making them work with disparate use cases from realtime to batch
processing, learning to use them effectively, and operating them in production is a
daunting task.

No integrated framework, numerous integration points
----------------------------------------------------

As an application developer, one of the main challenges of building a Big Data
application is that you have to focus not only on the application layer of code but also
on the the infrastructure layer. As highlighted above, you first make choices about the
underlying technology frameworks, then spend time integrating the different pieces of
technology together, all before you even start building your application. Each of the
technology frameworks come with their own APIs making it harder to integrate them quickly.

Lack of development tools
-------------------------
Big data application development involves dealing with technology frameworks in a
distributed system environment, and there is no development framework that makes it
easy to develop, test and debug these types of applications. Debugging is especially
difficult in a distributed environment. Sometimes you have no choice but to scan through
hundred of lines of log files on multiple systems to debug your application.

No monitoring solutions
-----------------------

Once your application is ready for production, you'll need to monitor and manage it.
Operability of each of the technology frameworks presents its own set of challenges.
A lack of proper tools makes application operations a full-time job.

Why is Apache Hadoop hard?
--------------------------

Though Apache Hadoop |(TM)| has demonstrated its value in many use-cases, developers spend too much time
working with infrastructure issues instead of their core concerns. As a consequence,
Hadoop will never be a mainstream technology for data applications unless that can be changed.

If you are building a data application and managing enterprise-scale data,
you will need to address:

- Consistency of data
- Scalability of performance and storage
- Monitoring and management
- Security of access and modification
- and so forth...

Consistency of data can be handled by using a platform providing transactions.
Scalability requires elastic scale.
Monitoring and management requires logging, metrics and a testing and debugging framework.
Security requires authentication and the handling of credentials.

To do all of this, you'll need to build an entire platform that provides an infrastructure for Hadoop.


Cask Data Application Platform Overview
=======================================
Under the covers, **Cask Data Application Platform (CDAP)** is a Java-based middleware solution that
abstracts the complexities and integrates the components of the Hadoop ecosystem (YARN, MapReduce,
HBase, Zookeeper, etc.). Simply stated, CDAP behaves like a modern-day application
server, distributed and scalable, sitting on top of a Hadoop distribution (such as CDH,
HDP, or Apache). It provides a programming framework and scalable runtime environment
that allows any Java developer to build Big Data applications without having to
understand all of the details of Hadoop.

Integrated Framework
--------------------
Without a Big Data middleware layer, a developer has to piece together multiple open
source frameworks and runtimes to assemble a complete Big Data infrastructure stack.
CDAP provides an integrated platform that makes it easy to create all the elements of
Big Data applications: collecting, processing, storing, and querying data. Data can be
collected and stored in both structured and unstructured forms, processed in real-time
and in batch, and results can be made available for retrieval, visualization, and
further analysis.

Simple APIs
-----------
CDAP aims to reduce the time it takes to create and implement applications
by hiding the complexity of these distributed technologies with a set of powerful yet
simple APIs. You don’t need to be an expert on scalable, highly-available system
architectures, nor do you need to worry about the low-level Hadoop and HBase APIs.

Full Development Lifecycle Support
----------------------------------
CDAP supports developers through the entire application development lifecycle:
development, debugging, testing, continuous integration and production. Using familiar
development tools such as *IntelliJ* and *Eclipse*, you can build, test and debug your
application right on your laptop with a *Standalone CDAP*. Utilize the application unit
test framework for continuous integration. Deploy it to a development cloud or production
cloud (*Distributed CDAP*) with a push of a button.

Easy Application Operations
---------------------------
Once your Big Data application is in production, CDAP is designed
specifically to monitor your applications and scale with your data processing needs:
increase capacity with a click of a button without taking your application offline. Use
the CDAP Console or RESTful APIs to monitor and manage the lifecycle and scale of your
application.

CDAP Components
===============
Now, let’s talk about the components within CDAP. There are four basic abstractions:

- `Streams <programming.html#streams>`__ for real-time data collection from any external system;
- `Flows <programming.html#flows>`__ for performing elastically scalable, real-time stream
  or batch processing;
- `Datasets <programming.html#datasets>`__ for storing data in simple and scalable ways without
  worrying about details of the storage schema; and
- `Procedures <programming.html#procedures>`__ for exposing data to external systems through
  stored queries.

These are grouped into Applications for configuring and packaging.

Applications are built in Java using the CDAP Core APIs. Once an application is
deployed and running, you can easily interact with it from virtually any external system
by accessing the Streams, Datasets, and Procedures using the Java APIs, RESTful or other
network protocols.

CDAP functions as a middle-tier application platform. As seen in the diagram below,
it provides an interface (using either HTTP RESTful or Java APIs) to clients through a Router
along with services and features that run inside YARN containers in Hadoop.

.. image:: _images/architecture_diagram_1.png
:width: 6in
   :align: center

  These services and features include:

  - **Dataset Abstractions:** with globally consistent transactions provided by using
  `Cask Tephra <http://github.com/continuuity/tephra/>`__, these
  abstractions provide generic, reusable Java implementations of common data patterns;

- **Streams:** the means for ingesting data from external systems in real time;

- **Realtime Processing using Flows:** developer-implemented, real-time Stream processors,
  comprised of one or more *Flowlets* wired together into a directed acyclic graph;

- **Batch Processing:** using MapReduce and Workflow Schedules, as in conventional
  Hadoop systems;

- **Ad-Hoc SQL Queries:** for datasets that implement methods for obtaining the schema
  and scanning the data record by record, you can access and query data using SQL;

- **Stored Procedures:** allowing synchronous calls into the Server from an external system
  and the performance of server-side processing on-demand, similar to a stored procedure in
  a traditional database;

- **Metrics, Logging and Monitoring:** system and user-defined metrics, along with standard
  SLF4J logs, a testing framework and the ability to attach a remote debugger;

- **Management Console:** available for deploying, querying and managing the Server;

- **Different Runtimes:** standalone (useful for learning, prototyping and testing) and
  Distributed versions;

- **YARN containers:** services are run in YARN containers in Hadoop, providing access to
  HBase and HDFS, giving the scalability and performance of Hadoop without the complexity.

In the next section, we will compare three application architectures and their pros and cons.
This will give you a good understanding of the benefit of architecting
Big Data applications using CDAP.

Architecture Comparison: Building A Big Data Application
============================================================
Consider the problem of building a real-time log analytic application that takes access
logs from Apache™ web servers and computes simple analyses on the logs, such as computing
throughput per second, error rates or finding the top referral sites.

Traditional Database Log Analysis Framework
-------------------------------------------
A traditional architecture will involve using a log collector (Custom ETL) that gathers
logs from different application servers or sources and then writing to a database. A
reporting framework OLAP/Reporting Engine) then acts as the processing layer to aggregate
the log signals into meaningful statistics and information.

This is a good example of an application architecture that cannot scale with unpredictable
volume and velocity of data. The custom ETL (extract, transform, load) framework includes
a log collector to extract data, transformation of the logs with simple filtering and
normalization, and performs the loading into the database of the events.

.. image:: _images/architecture_diagram_2.png
:width: 6in
   :align: center

  The disadvantages of this approach include:

  - Complexity of the application increases when processing large volumes of data
  - The architecture will not be horizontally scalable
  - Producing results in realtime at high-volume rates is challenging

Apache Hadoop®-based Log Analysis Framework
-------------------------------------------
To achieve horizontal scalability, the database architecture of the preceding design
has evolved to include scalable log collection, processing and storage layers.

One of the most commonly-used architectural patterns consists of
custom ETL and log aggregators using map reduce, a realtime stream processor such as
`Storm <http://storm-project.net>`__ as a data processing layer,
`Apache HDFS/HBase™ <http://hbase.apache.org>`__ as a storage layer of results
and a custom reporting engine reading the computed results and
creating visualizations for a web browser.
This is just a summary of the many components required to implement this solution.
(Don’t worry if you are not familiar with these technology frameworks.)

.. image:: _images/architecture_diagram_3.png
:width: 6in
   :align: center

  The disadvantages of this approach include:

  - Steep learning curve
  - Difficult to integrate different systems
  - Lack of development tools
  - Operating the composite software stack
  - No single unified architecture

CDAP Log Analysis Framework
------------------------------------------
Designing Big Data applications using the **Cask Data Application Platform** provides a clear
separation between infrastructure components and application code.

CDAP functions as a middle-tier application platform, exposing simple, high-level
abstractions to perform data collection, processing, storage and query. Logs are collected
by `Streams <programming.html#streams>`__, while `Flows <programming.html#flows>`__
do basic aggregation and realtime analysis. Advanced, off-line
aggregation is performed by `MapReduce <programming.html#mapreduce>`__ and
`Workflow <programming.html#workflows>`__ components.
`Procedures <programming.html#procedures>`__ provide
stored queries, with `Ad-hoc Queries <query.html>`__ accessing the data using SQL.
The application can now be scaled independent of the underlying infrastructure.

.. image:: _images/architecture_diagram_4.png
:width: 6in
   :align: center

  The advantages of this approach include:

  - A single unified architecture to perform data collection, processing, storage and query,
    with interoperability designed into the framework.
  - Horizontal scalability is derived from the underlying Apache Hadoop layer, while the
    **CDAP** APIs reduce the application complexity and development time.
