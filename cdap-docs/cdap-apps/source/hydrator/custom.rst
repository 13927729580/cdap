.. meta::
    :author: Cask Data, Inc.
    :copyright: Copyright © 2015 Cask Data, Inc.

.. _cdap-apps-custom-etl-plugins:

===========================
Creating Custom ETL Plugins
===========================

Overview
========
This section is intended for developers writing custom ETL plugins. Users of these should
refer to the :ref:`Included Applications <cdap-apps-index>`.

CDAP provides for the creation of custom ETL plugins for batch/real-time sources/sinks and
transformations to extend the existing ``cdap-etl-batch`` and ``cdap-etl-realtime`` system artifacts.


Plugin Types and Maven Archetypes
=================================
In ETL applications, there are five plugin types:

- Batch Source (*batchsource*)
- Batch Sink (*batchsink*)
- Real-time Source (*realtimesource*)
- Real-time Sink (*realtimesink*)
- Transformation (*transform*)

There are five corresponding Maven archetypes available for starting a plugin project.

Available Annotations
---------------------
These annotations may be used with the plugin classes:

- ``@Plugin``: The class to be exposed as a plugin needs to be annotated with the ``@Plugin``
  annotation and the type of the plugin should be specified (*source*, *sink*, *transformation*).
  By default, the plugin type will be ‘plugin’.

- ``@Name``: Annotation used to name the plugin as well as the properties in the
  Configuration class of the plugin.

- ``@Description``: Annotation used to add a description.

- ``@Nullable``: Annotation indicates that the specific configuration property is
  optional. Such a plugin class can be used without that property being specified.


Creating a Batch Source
=======================
A batch source plugin can be created from a Maven archetype. This command will create a
project for the plugin from the archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-batch-source-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

In order to implement a Batch Source (to be used in the ETL Batch artifact), you extend
the ``BatchSource`` class. You need to define the types of the KEY and VALUE that the Batch
Source will receive and the type of object that the Batch Source will emit to the
subsequent stage (which could be either a Transformation or a Batch Sink). After defining
the types, only one method is required to be implemented:

  ``prepareRun()``

.. rubric:: Methods

- ``prepareRun()``: Used to configure the Hadoop Job configuration (for example, set the
  ``InputFormatClass``).
- ``configurePipeline()``: Used to create any streams or datasets or perform any validation 
  on the application configuration that are required by this plugin.
- ``initialize()``: Initialize the Batch Source. Guaranteed to be executed before any call
  to the plugin’s ``transform`` method.
- ``transform()``: This method will be called for every input key-value pair generated by 
  the batch job. By default, the value is emitted to the subsequent stage.

Example::

  @Plugin(type = "batchsource")
  @Name("MyBatchSource")
  @Description("Demo Source")
  public class MyBatchSource extends BatchSource<LongWritable, String, String> {

    @Override
    public void prepareRun(BatchSourceContext context) {
      Job job = context.getHadoopJob();
      job.setInputFormatClass(...);
      // Other Hadoop job configuration related to Input
    }
  }


Creating a Batch Sink
=====================
A batch sink plugin can be created from this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-batch-sink-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

In order to implement a Batch Sink (to be used in the ETL Batch artifact), you extend the
``BatchSink`` class. Similar to a Batch Source, you need to define the types of the KEY and
VALUE that the Batch Sink will write in the Batch job and the type of object that it will
accept from the previous stage (which could be either a Transformation or a Batch Source).

After defining the types, only one method is required to be implemented:

  ``prepareRun()``

.. rubric:: Methods

- ``prepareRun()``: Used to configure the Hadoop Job configuration (for ex, set ``OutputFormatClass``).
- ``configurePipeline()``: Used to create any datasets or perform any validation 
  on the application configuration that are required by this plugin.
- ``initialize()``: Initialize the Batch Sink runtime. Guaranteed to be executed before
  any call to the plugin’s ``transform`` method.
- ``transform()``: This method will be called for every object that is received from the
  previous stage. The logic inside the method will transform the object to the key-value
  pair expected by the Batch Sink's output format. If you don't override this method, the
  incoming object is set as the Key and the Value is set to null.

Example::

  @Plugin(type = "batchsink")
  @Name("MyBatchSink")
  @Description("Demo Sink")
  public class MyBatchSink extends BatchSink<String, String, NullWritable> {

    @Override
    public void prepareRun(BatchSinkContext context) {
      Job job = context.getHadoopJob();
      job.setOutputFormatClass(...);
      // Other Hadoop job configuration related to Output
    }
  }


Creating a Real-Time Source
===========================
A real-time source plugin can be created from this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-realtime-source-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

The only method that needs to be implemented is:

	``poll()``

.. rubric:: Methods

- ``initialize()``: Initialize the real-time source runtime. Guaranteed to be executed
  before any call to the poll method. Usually used to setup the connection to external
  sources.
- ``configurePipeline()``: Used to create any streams or datasets or perform any validation 
  on the application configuration that are required by this plugin.
- ``poll()``: Poll method will be invoked during the run of the plugin and in each call,
  the source is expected to emit zero or more objects for the next stage to process. 
- ``destroy()``: Cleanup method executed during the shutdown of the Source. Could be used
  to tear down any external connections made during the initialize method.

Example::

  /**
   * Real-Time Source to poll data from external sources.
   */
  @Plugin(type = "realtimesource")
  @Name("Source")
  @Description("Real-Time Source")
  public class Source extends RealtimeSource<StructuredRecord> {

    private final SourceConfig config;

    public Source(SourceConfig config) {
      this.config = config;
    }

    /**
     * Config class for Source.
     */
    public static class SourceConfig extends PluginConfig {

      @Name("param")
      @Description("Source Param")
      private String param;
      // Note:  only primitives (included boxed types) and string are the types that are supported

    }
  
    @Nullable
    @Override
    public SourceState poll(Emitter<StructuredRecord> writer, SourceState currentState) {
      // Poll for new data
      // Write structured record to the writer
      // writer.emit(writeDefaultRecords(writer);
      return currentState;
    }

    @Override
    public void initialize(RealtimeContext context) throws Exception {
      super.initialize(context);
      // Get Config param and use to initialize
      // String param = config.param
      // Perform init operations, external operations etc.
    }

    @Override
    public void destroy() {
      super.destroy();
      // Handle destroy lifecycle
    }

    private void writeDefaultRecords(Emitter<StructuredRecord> writer){
      Schema.Field bodyField = Schema.Field.of("body", Schema.of(Schema.Type.STRING));
      StructuredRecord.Builder recordBuilder = StructuredRecord.builder(Schema.recordOf("defaultRecord", bodyField));
      recordBuilder.set("body", "Hello");
      writer.emit(recordBuilder.build());
    }
  }


Creating a Real-Time Sink
=========================
A real-time sink plugin can be created from this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-realtime-sink-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

The only method that needs to be implemented is:

 ``write()``

.. rubric:: Methods

- ``initialize()``: Initialize the real-time sink runtime. Guaranteed to be executed before
  any call to the ``write`` method. 
- ``configurePipeline()``: Used to create any datasets or perform any validation 
  on the application configuration that are required by this plugin.
- ``write()``: The write method will be invoked for a set of objects that needs to be
  persisted. A ``DataWriter`` object can be used to write data to CDAP streams and/or datasets.
  The method is expected to return the number of objects written; this is used for collecting
  metrics.
- ``destroy()``: Cleanup method executed during the shutdown of the Sink. 

Example::

  @Plugin(type = "realtimesink")
  @Name("Demo")
  @Description("Demo Real-Time Sink")
  public class DemoSink extends RealtimeSink<String> {

    @Override
    public int write(Iterable<String> objects, DataWriter dataWriter) {
      int written = 0;
      for (String object : objects) {
        written += 1;
        . . .
      }
      return written;
    }
  }


Creating a Transformation
=========================
In ETL applications, a transformation operation is applied on one object at a time,
converting it into zero or more transformed outputs. A Transformation plugin can be created
using this Maven archetype:

.. container:: highlight

  .. parsed-literal::
  
    |$| mvn archetype:generate \\
          -DarchetypeGroupId=co.cask.cdap \\
          -DarchetypeArtifactId=cdap-etl-transform-archetype \\
          -DarchetypeVersion=\ |release| \\
          -DgroupId=org.example.plugin

You can replace the groupId with your own organization, but it must not be ``co.cask.cdap``.

The only method that needs to be implemented is:

	``transform()``

.. rubric:: Methods

- ``initialize()``: Used to perform any initialization step that might be required during
  the runtime of the ``Transform``. It is guaranteed that this method will be invoked
  before the ``transform`` method.
- ``transform()``: This method contains the logic that will be applied on each
  incoming data object. An emitter can be used to pass the results to the subsequent stage
  (which could be either another Transformation or a Sink).
- ``destroy()``: Used to perform any cleanup before the plugin shuts down.

Below is an example of a ``DuplicateTransform`` that emits copies of the incoming record
based on the value in the record. In addition, a user metric indicating the number of
copies in each transform is emitted. The user metrics can be queried by using the CDAP 
:ref:`Metrics HTTP RESTful API <http-restful-api-metrics>`::


  @Plugin(type = "transform")
  @Name("Duplicator")
  @Description("Transformation Example that makes copies")

  public class DuplicateTransform extends Transform<StructuredRecord, StructuredRecord> {
  
  private final Config config;

    public static final class Config extends PluginConfig {
    
      @Name("count")
      @Description("Field that indicates number of copies to make")
      private String fieldName; 
    } 
  
    @Override
    public void transform(StructuredRecord input, Emitter<StructuredRecord> emitter) {
      Integer copies = input.get(config.fieldName);
      for (int i = 0; i < copies; i++) {
        emitter.emit(input);
      }
      getContext().getMetrics().count("copies", copies);
    }

    @Override
    public void destroy() {
    
    }
  }


Test Framework for Plugins
==========================

.. include:: ../../../developers-manual/source/testing/testing.rst
   :start-after: .. _test-framework-strategies-artifacts:
   :end-before:  .. _test-framework-validating-sql:

Additional information on unit testing with CDAP is in the Developers’ Manual section
on :ref:`Testing a CDAP Application <test-framework>`.


Source State in a Real-Time Source
==================================
Real-time plugins are executed in workers. During failure, there is the possibility that
the data that is emitted from the Source will not be processed by subsequent stages. In
order to avoid such data loss, SourceState can be used to persist the information about
the external source (for example, offset) if supported by the Source. 

In case of failure, when the poll method is invoked, the offset last persisted is passed
to the poll method, which can be used to fetch the data from the last processed point. The
updated Source State information is returned by the poll method. After the data is
processed by any Transformations and then finally persisted by the Sink, the new Source
State information is also persisted. This ensures that there will be no data loss in case
of failures.

::

  @Plugin(type = "realtimesource")
  @Name("Demo")
  @Description("Demo Real-Time Source")
  public class DemoSource extends RealtimeSource<String> {
    private static final Logger LOG = LoggerFactory.getLogger(TestSource.class);
    private static final String COUNT = "count";

    @Nullable
    @Override
    public SourceState poll(Emitter<String> writer, SourceState currentState) {
      try {
        TimeUnit.MILLISECONDS.sleep(100);
      } catch (InterruptedException e) {
        LOG.error("Some Error in Source");
      }

      int prevCount;
      if (currentState.getState(COUNT) != null) {
        prevCount = Bytes.toInt(currentState.getState(COUNT));
        prevCount++;
        currentState.setState(COUNT, Bytes.toBytes(prevCount));
      } else {
        prevCount = 1;
        currentState = new SourceState();
        currentState.setState(COUNT, Bytes.toBytes(prevCount));
      }

      LOG.info("Emitting data! {}", prevCount);
      writer.emit("Hello World!");
      return currentState;
    }
  }
  
  
.. _cdap-apps-custom-etl-plugins-plugin-packaging:

Plugin Packaging and Deployment
===============================
To package and deploy your plugin, follow these instructions on `plugin packaging <#plugin-packaging>`__,
`deployment <#deploying-a-system-artifact>`__ and `verification <#deployment-verification>`__.

By using one of the ``etl-plugin`` Maven archetypes, your project will be set up to generate
the required JAR manifest. If you move the plugin class to a different Java package after
the project is created, you will need to modify the configuration of the
``maven-bundle-plugin`` in the ``pom.xml`` file to reflect the package name changes.

If you are developing plugins for the ``cdap-etl-batch`` artifact, be aware that for
classes inside the plugin JAR that you have added to the Hadoop Job configuration directly
(for example, your custom ``InputFormat`` class), you will need to add the Java packages
of those classes to the "Export-Package" as well. This is to ensure those classes are
visible to the Hadoop MapReduce framework during the plugin execution. Otherwise, the
execution will typically fail with a ``ClassNotFoundException``.

Plugin Deployment
-----------------

.. include:: ../../../developers-manual/source/building-blocks/plugins.rst 
   :start-after: .. _plugins-deployment-artifact:
   :end-before:  .. _plugins-use-case:

Configuration JSON for a plugin (for Webapp UI alone)
======================================================
Every property of a plugin is represented, by default, as input fields in the UI. This can be however changed to specific widgets that closely match the type of the property in a plugin. This can be specified as JSON for the UI to consume.

The configuration JSON is composed of a list of property groups and a list of outputs. For instance the JSON would look like this,

.. code-block:: Javascript

  {
    "configuration-groups": [
      "group1",
      "group2",
      ...
    ],
    "outputs": [
      {"ouput-property-1"},
      {"ouput-property-2"},
      ...
    ]
  }

Configuration groups
---------------------
Configuration groups are simple grouping of properties in a plugin. For instance in a Stream Source plugin - Stream Name, Duration & Delay could be grouped as Stream Configuration. So based on this example a configuration group can be represented as an object with a name & a list of properties of the plugin that falls under that group. In the case of Stream source plugin it would look like this,

.. code-block:: Javascript

  {
    "configuration-groups": [
      {
        "name": "Stream Configuration",
        "properties": [
          {"field1"},
          {"field2"},
          {"field3"}
        ]
      }
    ],
    outputs: [
      {output-property1},
      {output-property2},
      ..
    ]
  }

Once a group is established we can configure how each field inside the group is represented in the UI. The configuration of each property of the plugin is composed of following parts:

  - name : Name of the field (as coming from the CDAP backend)
  - label: Label to be used in the UI for the property
  - widget-type : The type of widget that needs to represent this property
  - widget-attributes: A map of attributes that the widget shall require to be rendered in UI. The attributes depend on the type of widget.

In the case of Stream plugin this would look like this,

.. code-block:: Javascript

  {
    "configuration-groups": [
      {
        "name": "Stream Configuration",
        "properties": [
          {
            "name": "duration",
            "label": "Duration",
            "widget-type": "textbox"
          },
          {
            "widget-type": "textbox",
            "label": "Delay",
            "name": "delay"
          },
          {
            "widget-type": "stream-selector",
            "label": "Stream Name",
            "name": "name"
          }
        ]
      }
    ],
    "outputs": [
      {"output-property1"}
    ]
  }

A widget in UI represents a component that will be rendered and used to set a value of a property of a plugin. There are different widgets that we support in Hydrator as of version 3.3.

.. list-table::
   :widths: 20 25 25 25
   :header-rows: 1

   * - Widget-type
     - Description
     - Attributes
     - Output data type
   * - textbox
     - default - A default value for the widget
     - Default HTML textbox to enter any string
     - string
   * - number
     - | default - A default value for the widget
       | min - A min value for the number box
       | max - A max value for the number box
     - Default HTML number textbox. Can enter only valid numbers
     - string
   * - passwordbox
     - NA
     - Default html password box
     - string
   * - datetime
     - | default - A default value for the widget
       | format - format should be ISO, long, short or full format dates
     - Date time picker. Used to set date (in string) for any property
     - string
   * - csv/dsv
     - delimiter (allows to change the delimiter to be any other symbol apart from ‘,’)
     - Comma separated values. Each value is entered in a separate box
     - comma-separated string
   * - json-editor
     - default - A default value for the widget (stringified JSON)
     - Json editor to pretty-print and auto format JSON while typing
     - string
   * - javascript-editor,python-editor
     - default - A default value for the widget (string)
     - An editor to write Javscript or Python as a value for a property
     - string
   * - keyvalue
     - | delimiter - Delimiter for key-value pairs
       | kv-delimiter - Delimiter for key & value.
     - A key-value editor which allows you to construct map
     - string
   * - keyvalue-dropdown
     - Same as keyvalue attributes. dropdownOptions - a list of dropdown options to use in UI
     - It is exactly the keyvalue widget but instead of plain value we have dropdown with a list of values
     - string
   * - select
     - | values - List of values for the dropdown
       | default - A default value from the list
     - An html dropdown with a list of values. Allows to choose one from the list
     - string
   * - dataset-selector/stream-selector
     - NA
     - A typeahead textbox that will have a list of datasets in the cdap instance
     - string
   * - schema
     - | schema-types - A list of schema types for each field that the user can chose while setting the schema
       | schema-default-type - A default type for each newly added field in the schema
       | property-watch - A property of the plugin to watch which might affect the output schema
     - A 4 column editable table to represent schema in a plugin
     - string
   * - non-editable-schema-editor
     - schema - The schema that will be used as output schema for the plugin
     - A non-editable schema widget
     - string


Outputs
-------

The outputs is a list of plugin properties that represent the output schema for the particular plugin.

The output schema for a plugin can be represented in two different ways:
  - Via Implicit schema
  - Via Schema property

Implicit schema is a pre-determined output schema for a plugin that the plugin developer can enforce. The implicit schema is not associated with any property of the plugin but just enforces the output schema for the plugin (for visual purposes only).

A particular property of the plugin can be defined as the output schema and can be appropriately configured to make it editable in the UI.

An output-property is configured in exactly the same way we configure individual properties in configuration-groups. It is composed of name, widget-type & widget-attributes. However the subtle difference is the widget-type used for the output property. The widget-type for an output property is ideally 'schema' or 'non-editable-schema-editor'. This is to ensure the schema that is propagated across different plugins are consistent. The difference comes in 'non-editable-schema-editor' widget which doesn't have widget-attributes but a schema that will be used in the UI as output schema for the plugin.

Based on the above definitions we could write the configuration JSON for Stream source which has the following properties - schema, duration, name, format, delay - as

.. code-block:: Javascript

  {
    "metadata": {
      "spec-version": "1.0"
    },
    "configuration-groups": [
      {
        "label": "Stream Configuration",
        "properties": [
          {
            "widget-type": "textbox",
            "label": "Stream Name",
            "name": "name"
          },
          {
            "widget-type": "textbox",
            "label": "Duration",
            "name": "duration"
          },
          {
            "widget-type": "textbox",
            "label": "Delay",
            "name": "delay"
          },
          {
            "widget-type": "textbox",
            "label": "Data Field Name",
            "name": "body.field",
            "widget-attributes": {
              "width": "medium"
            }
          },
          {
            "widget-type": "textbox",
            "label": "Header Field Name",
            "name": "headers.field",
            "widget-attributes": {
              "width": "medium"
            }
          },
          {
            "widget-type": "stream-selector",
            "label": "Stream Name",
            "name": "name",
            "widget-attributes": {
              "width": "medium"
            }
          }
        ]
      },
      {
        "label": "Format",
        "properties": [
          {
            "widget-type": "select",
            "label": "Format",
            "name": "format",
            "widget-attributes": {
              "values": [
                "avro",
                "clf",
                "csv",
                "grok",
                "syslog",
                "text",
                "tsv"
              ],
              "default": "text"
            }
          }
        ]
      }
    ],
    "outputs": [
      {
        "name": "schema",
        "widget-type": "schema",
        "widget-attributes": {
          "schema-types": [
            "boolean",
            "int",
            "long",
            "float",
            "double",
            "string",
            "map<string, string>"
          ],
          "schema-default-type": "string",
          "property-watch": "format"
        }
      }
    ]
  }
