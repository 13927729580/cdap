/*
 * Copyright Â© 2014 Cask Data, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not
 * use this file except in compliance with the License. You may obtain a copy of
 * the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
 * License for the specific language governing permissions and limitations under
 * the License.
 */

package co.cask.cdap.notifications.kafka;

import co.cask.cdap.api.data.DatasetContext;
import co.cask.cdap.api.data.DatasetInstantiationException;
import co.cask.cdap.api.dataset.Dataset;
import co.cask.cdap.api.service.TxRunnable;
import co.cask.cdap.data2.dataset2.DatasetFramework;
import co.cask.cdap.data2.dataset2.DatasetManagementException;
import co.cask.cdap.notifications.NotificationContext;
import co.cask.cdap.notifications.NotificationFeed;
import co.cask.cdap.notifications.NotificationHandler;
import co.cask.cdap.notifications.NotificationSubscriber;
import co.cask.cdap.notifications.TxRetryPolicy;
import co.cask.cdap.notifications.client.NotificationFeedClient;
import co.cask.cdap.notifications.service.NotificationFeedException;
import co.cask.tephra.TransactionAware;
import co.cask.tephra.TransactionContext;
import co.cask.tephra.TransactionFailureException;
import co.cask.tephra.TransactionSystemClient;
import com.google.common.base.Throwables;
import com.google.common.collect.Maps;
import com.google.inject.Inject;
import org.apache.twill.common.Cancellable;
import org.apache.twill.kafka.client.FetchedMessage;
import org.apache.twill.kafka.client.KafkaClient;
import org.apache.twill.kafka.client.KafkaConsumer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.Iterator;
import java.util.Map;

/**
 * Kafka implementation of a {@link co.cask.cdap.notifications.NotificationPublisher}.
 */
public class KafkaNotificationSubscriber implements NotificationSubscriber {
  private static final Logger LOG = LoggerFactory.getLogger(KafkaNotificationSubscriber.class);

  private final KafkaClient kafkaClient;
  private final DatasetFramework dsFramework;
  private final NotificationFeedClient feedClient;
  private final TransactionSystemClient transactionSystemClient;

  @Inject
  public KafkaNotificationSubscriber(KafkaClient kafkaClient, DatasetFramework dsFramework,
                                     NotificationFeedClient feedClient,
                                     TransactionSystemClient transactionSystemClient) {
    this.kafkaClient = kafkaClient;
    this.dsFramework = dsFramework;
    this.feedClient = feedClient;
    this.transactionSystemClient = transactionSystemClient;
  }

  @Override
  public Preparer prepare() {
    final KafkaConsumer.Preparer kafkaPreparer = kafkaClient.getConsumer().prepare();
    final Map<NotificationFeed, NotificationHandler> feedMap = Maps.newConcurrentMap();

    return new Preparer() {
      @Override
      public <N> Preparer add(NotificationFeed feed, NotificationHandler<N> handler) throws NotificationFeedException {
        // This call will make sure that the feed exists
        feedClient.getFeed(feed);

        String topic = KafkaNotificationUtils.getKafkaTopic(feed);
        // TODO pay attention to that - we may miss notifications when the topic is empty, at the beginning
        kafkaPreparer.addFromBeginning(topic, 0); // TODO change to addLatest
        feedMap.put(feed, handler);
        return this;
      }

      @Override
      public Cancellable consume() {
        return kafkaPreparer.consume(new KafkaConsumer.MessageCallback() {
          @Override
          public void onReceived(Iterator<FetchedMessage> messages) {
            int count = 0;
            while (messages.hasNext()) {
              FetchedMessage message = messages.next();
              ByteBuffer payload = message.getPayload();

              try {
                for (Map.Entry<NotificationFeed, NotificationHandler> feedEntry : feedMap.entrySet()) {
                  Object notification = KafkaMessageSerializer.decode(feedEntry.getKey(), payload,
                                                                      feedEntry.getValue().getNotificationFeedType());
                  if (notification == null) {
                    continue;
                  }
                  feedEntry.getValue().processNotification(notification, new BasicNotificationContext());
                  count++;
                }
              } catch (IOException e) {
                // TODO What should be done here?...
                LOG.error("Could not decode Kafka message {}", message);
                Throwables.propagate(e);
              }
            }
            LOG.debug("Got {} messages from kafka", count);
          }

          @Override
          public void finished() {
            LOG.info("Subscription finished.");
          }
        });
      }
    };
  }

  /**
   * Implementation of {@link co.cask.cdap.notifications.NotificationContext}.
   */
  private final class BasicNotificationContext implements NotificationContext {

    @Override
    public boolean execute(TxRunnable runnable, TxRetryPolicy policy) {
      int countFail = 0;
      while (true) {
        try {
          final TransactionContext context = new TransactionContext(transactionSystemClient);
          try {
            context.start();
            runnable.run(new DynamicDatasetContext(context));
            context.finish();
            return true;
          } catch (TransactionFailureException e) {
            abortTransaction(e, "Failed to commit. Aborting transaction.", context);
          } catch (Exception e) {
            abortTransaction(e, "Exception occurred running user code. Aborting transaction.", context);
          }
        } catch (Throwable t) {
          switch (policy.handleFailure(++countFail, t)) {
            case RETRY:
              LOG.warn("Retrying failed transaction");
              break;
            case DROP:
              LOG.warn("Could not execute transactional operation.", t);
              return false;
          }
        }
      }
    }

    private void abortTransaction(Exception e, String message, TransactionContext context) {
      try {
        LOG.error(message, e);
        context.abort();
        throw Throwables.propagate(e);
      } catch (TransactionFailureException e1) {
        LOG.error("Failed to abort transaction.", e1);
        throw Throwables.propagate(e1);
      }
    }
  }

  /**
   * Implementation of {@link co.cask.cdap.api.data.DatasetContext} that allows to dynamically load datasets
   * into a started {@link TransactionContext}.
   */
  private final class DynamicDatasetContext implements DatasetContext {

    private final TransactionContext context;

    private DynamicDatasetContext(TransactionContext context) {
      this.context = context;
    }

    @Override
    public <T extends Dataset> T getDataset(String name) throws DatasetInstantiationException {
      return getDataset(name, null);
    }

    @Override
    public <T extends Dataset> T getDataset(String name, Map<String, String> arguments)
      throws DatasetInstantiationException {
      try {
        T dataset = dsFramework.getDataset(name, arguments, null);
        if (dataset instanceof TransactionAware) {
          context.addTransactionAware((TransactionAware) dataset);
        }
        return dataset;
      } catch (DatasetManagementException e) {
        throw new DatasetInstantiationException("Could not retrieve dataset metadata", e);
      } catch (IOException e) {
        throw new DatasetInstantiationException("Error when instantiating dataset", e);
      }
    }
  }
}
