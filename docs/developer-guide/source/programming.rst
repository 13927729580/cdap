.. :Author: John Jackson   :Description: Introduction to programming applications for the Continuuity Reactor   ===================================================Continuuity Reactor Programming Guide===================================================-----------------------------------------------------------------------Introduction to programming applications for the Continuuity Reactor-----------------------------------------------------------------------Introduction============This document covers in detail the Continuuity Reactor core elements—Streams, Datasets, Flows, Procedures, MapReduce, and Workflows—and how you work with them in Java to build a Big Data application.For a high-level view of the concepts of the Continuuity Reactor Java APIs, please see the `Introduction to Continuuity Reactor <intro.html>`_.Finally, the implementation of an example application is described to illustrate these concepts and show how an entire application is built.For more information beyond this document, see both the `Javadocs <url>`_ [DOCNOTE: FIXME!] and the example code in the `examples <url>`_ [DOCNOTE: FIXME!] directory, both of which are on the Continuuity.com `developer website <url>`_ [DOCNOTE: FIXME!] as well as in your Reactor installation directory.Conventions-----------In this document, *Application* refers to a user Application that has been deployed into the Continuuity Reactor.Text that are variables that you are to replace is indicated by a series of angle brackets (``< >``). For example::	PUT /v2/streams/<new-stream-id>indicates that the text ``<new-stream-id>`` is a variable and that you are to replace it with your value,perhaps in this case *mystream*::	PUT /v2/streams/mystreamProgramming APIs================Application-----------An Application is a collection of Streams, DataSets, Flows, Procedures, MapReduce, and Workflows. To create an Application, implement the Application interface; you specify the Application metadata and declare and configure each Application element::	public class MyApp implements Application {	  @Override	  public ApplicationSpecification configure() {	    return ApplicationSpecification.Builder.with()	      .setName("myApp")	      .setDescription("my sample app")	      .withStreams()	        .add(...) ... 	      .withDataSets()	        .add(...) ... 	      .withFlows()	        .add(...) ...	      .withProcedures()	        .add(...) ...	      .withMapReduce()	        .add(...) ...	      .withWorkflows()	        .add(...) ...	      .build();	  }	}All elements must either be specified, or alternatively, you can specify that an Applicationdoes not use a particular element, for example, a Stream, by using the ``.no...`` methods::	      ...	      .setDescription("my sample app")	      .noStream()	      .withDataSets()	        .add(...) ...and so forth for all of the elements.Collect-------Streams.......Streams are the primary means for bringing data into the Continuuity Reactor. You specify a Stream in your `Application`_ metadata::	.withStreams()	  .add(new Stream("myStream")) ...Process-------			Flows.....**Flows** are composed of connected Flowlets wired into a DAG. To create a Flow, implement the Flow interface.This allows you to specify the Flow’s metadata, Flowlets, Flowlet connections, Stream to Flowlet connections,and any DataSets used in the Flow via a ``FlowSpecification`` using ``FlowSpecification.Builder()``::	class MyExampleFlow implements Flow {	  @Override	  public FlowSpecification configure() {	    return FlowSpecification.Builder.with()	      .setName("mySampleFlow")	      .setDescription("Flow for showing examples")	      .withFlowlets()	        .add("flowlet1", new MyExampleFlowlet())	        .add("flowlet2", new MyExampleFlowlet2())	      .connect()	        .fromStream("myStream").to("flowlet1")	        .from("flowlet1").to("flowlet2")	      .build();	}Flowlets........**Flowlets**, the basic building blocks of a Flow, represent each individual processing node within a Flow. Flowlets consume data objects from their inputs and execute custom logic on each data object, allowing you to perform data operations as well as emit data objects to the Flowlet’s outputs. Flowlets specify an ``initialize()`` method, which is executed at the startup of each instance of a Flowlet before it receives any data.The example below shows a Flowlet that reads *Double* values, rounds them, and emits the results. It has a simple configuration method and does nothing for initialization and destruction::	class RoundingFlowlet implements Flowlet {	  @Override	  public FlowletSpecification configure() { 	    return FlowletSpecification.Builder.with().	      setName("round").	      setDescription("A rounding Flowlet").	      build();	  }	  @Override	    public void initialize(FlowletContext context) throws Exception {	  }	  @Override	  public void destroy() { 	  }The most interesting method of this Flowlet is ``round()``, the method that does the actual processing. It uses an output emitter to send data to its output. This is the only way that a Flowlet can emit output::	OutputEmitter<Long> output;	@ProcessInput	public void round(Double number) {	  output.emit(Math.round(number));	}Note that the Flowlet declares the output emitter but does not initialize it. The Flow system injects its implementation at runtime.The method is annotated with @ProcessInput – this tells the Flow system that this method can process input data.You can overload the process method of a Flowlet by adding multiple methods with different input types. When an input object comes in, the Flowlet will call the method that matches the object’s type::	OutputEmitter<Long> output;	@ProcessInput	public void round(Double number) {	  output.emit(Math.round(number));	}	@ProcessInput	public void round(Float number) {	  output.emit((long)Math.round(number));	}If you define multiple process methods, a method will be selected based on the input object’s origin; that is, the name of a Stream or the name of an output of a Flowlet. A Flowlet that emits data can specify this name using an annotation on the output emitter. In the absence of this annotation, the name of the output defaults to “out”::	@Output("code")	OutputEmitter<String> out;Data objects emitted through this output can then be directed to a process method of the receiving Flowlet by annotating the method with the origin name::	@ProcessInput("code")	public void tokenizeCode(String text) {	  ... // perform fancy code tokenization	}Input context`````````````A process method can have an additional parameter, the ``InputContext``. The input context provides information about the input object, such as its origin and the number of times the object has been retried. For example, this Flowlet tokenizes text in a smart way and uses the input context to decide which tokenizer to use::	@ProcessInput	public void tokenize(String text, InputContext context) throws Exception {	  Tokenizer tokenizer;	  // if this failed before, fall back to simple white space	  if (context.getRetryCount() > 0) {	    tokenizer = new WhiteSpaceTokenizer();	  }	  // is this code? If its origin is named "code", then assume yes 	  else if ("code".equals(context.getOrigin())) {	    tokenizer = new CodeTokenizer();	  }	  else {	    // use the smarter tokenizer	    tokenizer = new NaturalLanguageTokenizer();	  }	  for (String token : tokenizer.tokenize(text)) {	    output.emit(token);	  }	}Type projection```````````````Flowlets perform an implicit projection on the input objects if they do not match exactly what the process method accepts as arguments. This allows you to write a single process method that can accept multiple **compatible** types. For example, if you have a process method::	@ProcessInput	count(String word) {	  ... 	}and you send data of type ``Long`` to this Flowlet, then that type does not exactly match what the process method expects. You could now write another process method for ``Long`` numbers:	@ProcessInput count(Long number) {	count(number.toString());	}and you could do that for every type that you might possibly want to count, but that would be rather tedious. Type projection does this for you automatically. If no process method is found that matches the type of an object exactly, it picks a method that is compatible with the object.In this case, because Long can be converted into a String, it is compatible with the original process method. Other compatible conversions are:- Every primitive type that can be converted to a ``String`` is compatible with ``String``.- Any numeric type is compatible with numeric types that can represent it.  For example, ``int`` is compatible with ``long``, ``float`` and ``double``,  and ``long`` is compatible with ``float`` and ``double``, but ``long`` is not   compatible with ``int`` because ``int`` cannot represent every ``long`` value.- A byte array is compatible with a ``ByteBuffer`` and vice versa.- A collection of type A is compatible with a collection of type B,  if type A is compatible with type B.   Here, a collection can be an array or any Java ``Collection``.   Hence, a ``List<Integer>`` is compatible with a ``String[]`` array.- Two maps are compatible if their underlying types are compatible.   For example, a ``TreeMap<Integer, Boolean>`` is compatible with a ``HashMap<String, String>``.- Other Java objects can be compatible if their fields are compatible.  For example, in the following class ``Point`` is compatible with ``Coordinate``,   because all common fields between the two classes are compatible.   When projecting from ``Point`` to ``Coordinate``, the color field is dropped,   whereas the projection from ``Coordinate`` to ``Point`` will leave the ``color`` field as ``null``::	class Point {	  private int x;	  private int y;	  private String color;	}	class Coordinates { 	  int x;	  int y;	}Type projections help you keep your code generic and reusable. They also interact well with inheritance. If a Flowlet can process a specific object class, then it can also process any subclass of that class.Stream event````````````A Stream event is a special type of object that comes in via Streams. It consists of a set of headers represented by a map from String to String, and a byte array as the body of the event. To consume a Stream with a Flow, define a Flowlet that processes data of type ``StreamEvent``::	class StreamReader extends AbstractFlowlet {	  ...	  @ProcessInput	  public void processEvent(StreamEvent event) {	    ... 	  }Flowlet method @Tick annotation```````````````````````````````A Flowlet’s method can be annotated with @Tick. Instead of processing data objects from a flowlet input, this method is invoked periodically, without arguments. This can be used, for example, to generate data, or pull data from an external data source periodically on a fixed cadence.In this code snippet from the CountRandom example, the @Tick method in the flowlet emits random numbers::	public class RandomSource extends AbstractFlowlet { 		  private OutputEmitter<Integer> randomOutput; 		  private final Random random = new Random();		  @Tick(delay = 1L, unit = TimeUnit.MILLISECONDS) 	  public void generate() throws InterruptedException {	    randomOutput.emit(random.nextInt(10000));	  }	}Connection``````````There are multiple ways to connect the Flowlets of a Flow. The most common form is to use the Flowlet name. Because the name of each Flowlet defaults to its class name, when building the flow specification you can simply do::	.withFlowlets()	  .add(new RandomGenerator()) 	  .add(new RoundingFlowlet())	.connect() 	  .fromStream("RandomGenerator").to(“RoundingFlowlet”)If you have two Flowlets of the same class, you can give them explicit names:	.withFlowlets()	  .add("random", new RandomGenerator())	  .add("generator", new RandomGenerator())	  .add("rounding", new RoundingFlowlet())	.connect()	  .fromStream("random").to("rounding")MapReduce.........To process data using MapReduce, specify ``withMapReduce()`` in your Application specification::	public ApplicationSpecification configure() {	return ApplicationSpecification.Builder.with()	   ...	   .withMapReduce()	     .add(new WordCountJob())	   ...You must implement the ``MapReduce`` interface, which requires the three methods:- ``configure()``,- ``beforeSubmit()``, and- ``onFinish()``.::	public class WordCountJob implements MapReduce {	  @Override	  public MapReduceSpecification configure() {	    return MapReduceSpecification.Builder.with()	      .setName("WordCountJob")	      .setDescription("Calculates word frequency")	      .useInputDataSet("messages")	      .useOutputDataSet("wordFrequency")	      .build();	  }The configure method is similar to the one found in Flow and Application. It defines the name and description of the MapReduce job. You can also specify DataSets to be used as input or output for the job.The ``beforeSubmit()`` method is invoked at runtime, before the MapReduce job is executed. Through a passed instance of the ``MapReduceContext`` you have access to the actual Hadoop job configuration, as though you were running the MapReduce job directly on Hadoop. For example, you can specify the mapper and reducer classes as well as the intermediate data format::	@Override	public void beforeSubmit(MapReduceContext context) throws Exception {	  Job job = context.getHadoopJob();	  job.setMapperClass(TokenizerMapper.class);	  job.setReducerClass(IntSumReducer.class);	  job.setMapOutputKeyClass(Text.class);	  job.setMapOutputValueClass(IntWritable.class);	}The ``onFinish()`` method is invoked after the MapReduce job has finished. You could perform cleanup or send a notification of job completion, if that was required. Because many MapReduce jobs do not need this method, the ``AbstractMapReduce`` class provides a default implementation that does nothing::	@Override	public void onFinish(boolean succeeded, MapReduceContext context) {	  // do nothing	}Continuuity Reactor ``Mapper`` and ``Reducer`` implement the standard Hadoop APIs::	public static class TokenizerMapper	    extends Mapper<byte[], byte[], Text, IntWritable> {		  private final static IntWritable one = new IntWritable(1); 	  private Text word = new Text();	  public void map(byte[] key, byte[] value, Context context)	      throws IOException, InterruptedException {	    StringTokenizer itr = new StringTokenizer(Bytes.toString(value)); 	    while (itr.hasMoreTokens()) {	      word.set(itr.nextToken());	      context.write(word, one);	    }	  }	}		public static class IntSumReducer	    extends Reducer<Text, IntWritable, byte[], byte[]> {		  public void reduce(Text key, Iterable<IntWritable> values, Context context)	      throws IOException, InterruptedException {	    int sum = 0;	    for (IntWritable val : values) {	      sum += val.get();	    }	    context.write(key.copyBytes(), Bytes.toBytes(sum));	  }	}MapReduce and DataSets``````````````````````Both Continuuity Reactor ``Mapper`` and ``Reducer`` can directly read from a DataSet or write to a DataSet similar to the way a Flowlet or Procedure can.To access a DataSet directly in Mapper or Reducer, you need to:- Declare the DataSet in the MapReduce job’s configure() method.   For example, to have access to a DataSet named *catalog*::	public class MyMapReduceJob implements MapReduce {	  @Override	  public MapReduceSpecification configure() {	    return MapReduceSpecification.Builder.with()	      ...	    .useDataSet("catalog")	      ...- And, inject the DataSet into the mapper or reducer that uses it::	public static class CatalogJoinMapper extends Mapper<byte[], Purchase, ...> {	  @UseDataSet("catalog")	  private ProductCatalog catalog;		  @Override	  public void map(byte[] key, Purchase purchase, Context context)	      throws IOException, InterruptedException {	    // join with catalog by product ID	    Product product = catalog.read(purchase.getProductId());	    ...	  }Workflows.........To process one or more MapReduce jobs in sequence, specify withWorkflows() in your application::	public ApplicationSpecification configure() {	  return ApplicationSpecification.Builder.with()	    ... 	    .withWorkflows()	      .add(new PurchaseHistoryWorkflow())You must implement the Workflow interface, which requires the configure() method. Use the addSchedule() method to run a workflow job periodically::	public static class PurchaseHistoryWorkflow implements Workflow {		  @Override	  public WorkflowSpecification configure() {	    return WorkflowSpecification.Builder.with()	      .setName("PurchaseHistoryWorkflow")	      .setDescription("PurchaseHistoryWorkflow description")	      .startWith(new PurchaseHistoryBuilder())	      .last(new PurchaseTrendBuilder())	      .addSchedule(new DefaultSchedule("FiveMinuteSchedule", "Run every 5 minutes",	                   "0/5 * * * *", Schedule.Action.START))	      .build();	  }	}	If there is only one MapReduce job to be run as a part of a workflow, use the onlyWith() method after setDescription() when building the Workflow::	public static class PurchaseHistoryWorkflow implements Workflow {	  @Override	  public WorkflowSpecification configure() {	    return WorkflowSpecification.Builder.with() .setName("PurchaseHistoryWorkflow")	      .setDescription("PurchaseHistoryWorkflow description")	      .onlyWith(new PurchaseHistoryBuilder())	      .addSchedule(new DefaultSchedule("FiveMinuteSchedule", "Run every 5 minutes",	                   "0/5 * * * *", Schedule.Action.START))	      .build();	  }	}Store-----DataSets........DataSets store and retrieve data. If your Application uses a DataSet, you must declare it in the Application specification. For example, to specify that your Application uses a ``KeyValueTable`` DataSet named *myCounters*, write::	public ApplicationSpecification configure() { 	  return ApplicationSpecification.Builder.with()	    ...	    .withDataSets().add(new KeyValueTable("myCounters"))	    ...To use the DataSet in a Flowlet or a Procedure, instruct the runtime system to inject an instance of the DataSet with the @UseDataSet annotation::	Class MyFowlet extends AbstractFlowlet {	  @UseDataSet("myCounters")	  private KeyValueTable counters; 	  ...	  void process(String key) {	    counters.increment(key.getBytes());	  }The runtime system reads the DataSet specification for the key/value table *myCounters* from the metadata store and injects a functional instance of the DataSet class into the Application.You can also implement custom DataSets by extending the ``DataSet`` base class or by extending existing DataSet types.Query-----Procedures..........Procedures receive calls from external systems and perform arbitrary server-side processing on demand.To create a Procedure, implement the Procedure interface, or more conveniently, extend the ``AbstractProcedure`` class. A Procedure is configured and initialized similarly to a Flowlet, but instead of a process method you’ll define a handler method. Upon external call, the handler method receives the request and sends a response. The most generic way to send a response is to obtain a Writer and stream out the response as bytes. Make sure to close the Writer when you are done::	class HelloWorld extends AbstractProcedure {	  @Handle("hello")	  public void wave(ProcedureRequest request,	                   ProcedureResponder responder) throws IOException {	    String hello = "Hello " + request.getArgument("who");	    ProcedureResponse.Writer writer = 	      responder.stream(new ProcedureResponse(SUCCESS));	    writer.write(ByteBuffer.wrap(hello.getBytes())).close();	  }	}This uses the most generic way to create the response, which allows you to send arbitrary byte content as the response body. In many cases, you will actually respond with JSON. Continuuity Reactor Procedures have convenience methods for return JSON maps::	// return a JSON map	Map<String, Object> results = new TreeMap<String, Object>();	results.put("totalWords", totalWords);	results.put("uniqueWords", uniqueWords);	results.put("averageLength", averageLength);	responder.sendJson(results);There is also a convenience method to respond with an error message::	@Handle("getCount")	public void getCount(ProcedureRequest request, ProcedureResponder responder) {	  String word = request.getArgument("word"); 	  if (word == null) {	    responder.error(Code.CLIENT_ERROR,	                    "Method 'getCount' requires argument 'word'");	    return;	  }[DOCNOTE: FIXME!] Shouldn't getCount throws IOException?Testing [rev 2]===============Strategies in testing applications----------------------------------Unit testing------------Local Continuuity Reactor-------------------------.. include:: includes/footer.rst
