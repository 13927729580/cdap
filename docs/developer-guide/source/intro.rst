.. :Author: John Jackson   :Date: $Date: 2014-01-21 $   :Description: Introduction to Continuuity Reactor.. toctree::   :maxdepth: 2===================================Introduction to Continuuity Reactor===================================What is a Big Data application?-------------------------------An application that can process a large data set, with the size of data sets measured in terabytes, petabytes and larger.Big Data applications scale resources linearly with a linear increase in the amount of data that is being processed.Challenges in building big data applications............................................The main challenge in building a Big Data application is that an application developerhas to focus not just on the application layer of code -- the business logic -- but also on the infrastructure layer, making technical decisions about the underlying technology frameworks, which ones to use and how to integrate them together into an effective application.With the current set of platforms, an application developer aiming to solve a simple log analytic business problem needs to pull together multiple technologies. Broadly speaking, the developer will be concerned with developing four areas:1. Data collection framework2. Data processing framework3. Data storage framework4. Data serving frameworkThere are a number of different challenges present in each of the frameworks mentioned above. To make the problem harder, the integration between these frameworks and the operability of the application that is stitched together is itself a challenge.Architecture comparison: building a log analytic application------------------------------------------------------------Consider a problem of building a real­time log analytic application that takes access log from Apache servers and computes simple analyses on the logs -- such as computing throughput per second, error rates, finding the top referral sites.Traditional database log analysis framework...........................................A traditional architecture that is not based on Hadoop will involve using a log collector that gathers logs from different application servers and then writes to a database, either flat-file or relational. A reporting framework acts as the processing layer to crunch the log signals into meaningful statistics and information.The disadvantages of this approach include:- Complexity of the application increases when processing large volumes of data- The architecture will not be horizontally scalable- Producing results in a real­time at high volume rate will prove quite challenging.. image:: images/ArchitectureDiagram_1.pngReal­time Hadoop-based log analysis framework.............................................To achieve horizontal scalability, the database architecture of the preceding design has evolved to include scalable log collection, processing and storage layers. One of the most commonly used architectural patterns consists of Apache Kafka as the distributed log collection framework, Storm as the data processing layer, Apache HBase as the storage layer of results and a custom serving layer reading the computed results for visualization by a presentation layer.The disadvantages of this approach include:- Need to integrate different systems- Operability of the different software stack- No single unified architecture- Large number of different layers.. image:: images/ArchitectureDiagram_2.pngContinuuity Reactor log analysis framework..........................................Using Continuuity Reactor as the application framework greatly simplifies the architecture. Continuuity Reactor exposes high­level abstractions to perform data collection, processing, storage and serving. There is a single unified architecture to perform these four tasks, with interoperability designed into the framework. Horizontal scalability is derived from the underlying Apache Hadoop layer, while the Continuuity Reactor APIs reduce the application complexity and development time... image:: images/ArchitectureDiagram_3.png.. [DOCNOTE: see also https://wiki.continuuity.com/pages/viewpage.action?pageId=12944176 andreas' JAX 2013 talk].. [DOCNOTE: See "Introducing Continuuity Reactor"].. [DOCNOTE: See "Continuuity Reactor Developer Guide"]Continuuity Reactor Overview----------------------------The Continuuity Reactor aims to minimize the pain points in big data application development by providing a unified infrastructure that can perform data collection, processing, storage and serving frameworks.Continuuity Reactor is a Java-based, integrated data and application framework that layers on top of Apache Hadoop®, Apache HBase, and other Hadoop ecosystem components. It surfaces the capabilities of the underlying infrastructure through simple Java and REST interfaces and shields you from unnecessary complexity. Rather than piecing together different open source frameworks and runtimes to assemble your own Big Data infrastructure stack, the Reactor provides an integrated platform that makes it easy to create the different elements of your Big Data application: collecting, processing, storing, and querying data.Continuuity Reactor Architecture.................................. image:: images/ReactorArchitecture.pngIn this diagram, you can see Continuuity Reactor's three layers and four elements. The three layers (bottom to top) are the Distributed Data Fabric, the Unified API, and the Continuuity SDK.In the Unified API are four elements (left to right): Collect, for Data Collection; Process, for Data Processing; Store, for Data Storage; and Query, for Data Query.The Continuuity Reactor is a unified Big Data application platform that brings various Big Data capabilities into a single environment and provides an elastic runtime for applications. Data can be ingested and stored in both structured and unstructured forms, then processed in real-time or in batch, and the results can be made available for retrieval.Continuuity Reactor Infrastructure components............................................... [DOCNOTE: Hadoop/Hbase with brief intro]Continuuity Reactor runs on top of Apache Hadoop. Apache Hadoop is a free, open source technology that runs on commodity hardware. Its distributed file system makes it inexpensive to store large amounts of data, and its scalable MapReduce analysis engine makes it possible to extract insights from that data. MapReduce is a variation of batch-driven data analysis, where the input data is partitioned into smaller batches that can be processed in parallel across many machines in a Hadoop cluster. However, MapReduce, while powerful enough to express many data analysis algorithms, is not always the optimal choice of programming paradigm, and often there is desire to run other computations on the Hadoop cluster, such as ad-hoc queries, real-time stream processing, message-passing (MPI), and distributed testing, to name a few.It is possible to run non-MapReduce computations on a Hadoop cluster, if you can "disguise" your computation as a MapReduce job. Recent innovations added to Apache Hadoop, such as Apache Yarn and Apache Twill, make these computations possible, but still require significant learning and development time. Continuuity Reactor aims to reduce the time it takes to create and implement applications by hiding the complexity of these technologies  with a set of powerful and simple APIs.Continuuity Reactor Core........................Brief Intro to Data fabric and App fabric.. [DOCNOTE: Need info here]Continuuity Reactor Elements.............................. [DOCNOTE: Brief intro to Reactor elements].. image:: images/ReactorArchitectureInternal.pngThe Reactor provides Streams for real-time data ingestion from any external system, Processors for performing elastically scalable real-time stream or batch processing, DataSets for storing data in simple and scalable ways without worrying about formats and schema, and Procedures for exposing data to external systems through interactive queries. These are grouped into Applications for configuring and packaging.You’ll build applications in Java using the Continuuity Core APIs. Once your application is deployed and running, you can easily interact with it from virtually any external system by accessing the streams, data sets, and procedures using REST or other network protocols.Application [Overview?] [apps run inside the main Application] collection of all these - Collect Data- Process Data- Store Data- Query Data- AppsIntroduction to Reactor Components----------------------------------Applications............Data Collection : Streams.........................Streams are the primary means for bringing data from external systems into the Reactor in real time. You can write to streams easily using REST or command line tools, either one operation at a time or in batches. Each individual signal sent to a stream is stored as an Event, which is comprised of a body (a blob of arbitrary binary data) and headers (a map of strings for metadata).Streams are identified by a Unique Stream ID string and must be explicitly created before being used. They can be created using a command line tool, the Management Dashboard, or programmatically within your application. Data written to a stream can be consumed by flows and processed in real-time as described below.		Purpose		Example [DOCNOTE: are these just the sample code blocks?]Data Processing: Flows......................Flows are developer-implemented real-time stream processors. [DOCNOTE: can we expand on this?] They are comprised of one or more Flowlets that are wired together into a directed acyclic graph (DAG). Flowlets pass Data Objects between one another. Each flowlet is able to perform custom logic and execute data operations for each individual data object processed. All data operations happen in a consistent and durable way.Flows are deployed to the Reactor and hosted within containers. Each flowlet instance runs in its own container. Each flowlet in the DAG can have multiple concurrent instances, each consuming a partition of the flowlet’s inputs.To put data into your flow, you can either connect the input of the flow to a stream, or you can implement a flowlet to generate data or pull it from an external source.		ExampleData Processing: Batch: MapReduce.................................		MapReduce is used to process data in batch. MapReduce jobs can be written the same way as in a conventional Hadoop system. In addition, Reactor DataSets can be accessed from MapReduce jobs as both input and output. 		ExampleData Processing: Batch: Workflows.................................		Workflows are used to execute a series of MapReduce jobs. A Workflow is given a sequence of jobs that follow each other, with an optional schedule to run the Workflow periodically. On successful execution of a job, the control is transferred to the next job in sequence until the last job in the sequence is executed. On failure, the execution is stopped at the failed job and no subsequent jobs in the sequence are executed.		ExampleData Storage: DataSets............	The core dataset of the Reactor is a Table. Unlike relational database systems, these tables are not organized into rows with a fixed schema. They are optimized for efficient storage of semi-structured data, data with unknown or variable schema, and sparse data.Other datasets are built on top of tables. A DataSet can implement specific semantics around a table, such as a key/value table or a counter table. A dataset can also combine multiple datasets into a more complex data pattern. For example, an indexed table can be implemented using one table for the data to index and a second table for the index.You can implement your own data patterns as custom datasets on top of tables. Because a number of useful datasets, including key/value tables, indexed tables and time series are already included with the Reactor, we call them system datasets.A number of useful datasets -- called system DataSets -- are included with Reactor, including key/value tables, indexed tables and time series.		Purpose		ExampleData Query ..........	Procedures		Purpose		ExampleExample Applications--------------------Basic: Logger with Streams, Flows, DataSets and Procedures..........................................................https://github.com/continuuity/reactor-apps/tree/develop/logger- Review and rewrite?- Converted to reSTIntermediate: Logger with MapReduce..................................... Advanced: Logger with Metrics and Application Logging [DOCNOTE: rev 2].. .........................................................Where To Go Next----------------- Continuuity.com- Download Continuuity Reactor- Developer Examples- Developer Guide- Support